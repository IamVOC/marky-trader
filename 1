import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Импортируем среду gym-trading-env
import gym_trading_env

# Определяем функцию вознаграждения (используем Sharpe Ratio)
def reward_function(portfolio_value):
    return (portfolio_value.mean() - rf) / portfolio_value.std()

# Создаем среду
env = gym_trading_env.make("BTC-USDT-1h")

# Определяем нейронную сеть политики
class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(env.observation_space.shape[0], 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, env.action_space.n)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.softmax(self.fc3(x))
        return x

# Определяем алгоритм обучения
def train(env, actor, lr=0.001):
    optimizer = optim.Adam(actor.parameters(), lr=lr)
    for episode in range(1000):
        state = env.reset()
        episode_return = 0
        episode_log_probs = []
        done = False
        while not done:
            state = torch.FloatTensor(state)
            action_probs = actor(state)
            action = torch.argmax(action_probs).item()
            next_state, reward, done, _ = env.step(action)
            episode_return += reward
            episode_log_probs.append(torch.log(action_probs[action]))
            state = next_state
        
        J = episode_return
        policy_gradient = []
        for log_prob in reversed(episode_log_probs):  
            policy_gradient.append(-log_prob * J)
            J = J * gamma 
        
        actor_loss = torch.stack(policy_gradient).sum()
        optimizer.zero_grad()
        actor_loss.backward()
        optimizer.step()

# Создаем модель актора
actor = Actor()

# Обучаем модель
train(env, actor)